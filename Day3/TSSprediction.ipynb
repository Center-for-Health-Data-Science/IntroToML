{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkSNDRdSWm5O"
   },
   "source": [
    "# Exercise: Predicting Human Transcription start sites\n",
    "\n",
    "In this exercise, we will work with a set of **human promoter sequences** and try to predict whether a location is a **Transcription Start Site (TSS)**. \n",
    "\n",
    "A TSS is the location where the first DNA nucleotide is transcribed into RNA, while a [promoter](https://www.genome.gov/genetics-glossary/Promoter) is a region of DNA upstream of a gene where proteins involved in the transcription process bind so that the gene's transcription can begin.\n",
    "\n",
    "In order to perform our predictive task, we will take advantage of a neural network.\n",
    "\n",
    "In detail, the neural network will take a \"window\" around a position in the DNA and predict whether the position is a transcription start site. Since the network will answer a yes/no question about the position, our task is a binary classification task.\n",
    "\n",
    "Therefore, we will also learn how to encode a DNA sequence to input to a neural network.\n",
    "\n",
    "Furthermore, we will see how convolution can be applied to this classification problem.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Throughout this exercise, we will answer a series of questions:\n",
    "\n",
    "### 1 Is the network over-fitting?\n",
    "\n",
    "First, we want to see whether the network we build is overfitting. Indeed, an over-fitting network fits the training data very well but generalizes very poorly on sets of new data points (namely points that the network has not seen during training) and is, therefore, an undesirable model.\n",
    "\n",
    "### 2 Which network performs best between a fully connected network and a convolutional network?\n",
    "\n",
    "Since we are trying to solve our prediction problem by testing out both a fully connected network and a convolutional network, we want to see which one performs best.\n",
    "\n",
    "### 3 Does the convolutional network improve when properly tweaking its architecture/parameters?\n",
    "\n",
    "We will analyze the impact of various changes on the convolutional network to assess whether any improves the network's performance.\n",
    "\n",
    "In detail, we will modify:\n",
    "\n",
    "* The size of the input\n",
    "* The network's parameters\n",
    "* The number of hidden units\n",
    "* The number of hidden layers\n",
    "\n",
    "### 4 What do you think about the network's performance?\n",
    "\n",
    "In your opinion, is the network's performance impressive/good/poor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDN1s6SQ4AkF"
   },
   "source": [
    "## Let's get started!\n",
    "\n",
    "### #1 Download the data\n",
    "\n",
    "We will work with human sequences from the EPD database. We download the data from Zenodo using `wget`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/records/11057936/files/EPD_all_hg38.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the sequences containing 999 bases before the TSS and 1000 bases after the TSS and use the FASTA file containing them. In a FASTA file, each sequence (= a string of letters representing the nucleotides in the sequence) is preceded by a line starting with a greater-than sign (`>`) that contains information about the sequence.\n",
    "\n",
    "These sequences will be our \"positive\" examples.\n",
    "\n",
    "We will use random positions in the same sequences as \"negative\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3256,
     "status": "ok",
     "timestamp": 1653202711102,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "BglOsx_q1NJq",
    "outputId": "3b594b1e-6b18-410c-ce86-fca5f775f018"
   },
   "outputs": [],
   "source": [
    "# We set the path to our FASTA file.\n",
    "fasta_file = \"EPD_all_hg38.fa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2 Install ``biopython``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5174,
     "status": "ok",
     "timestamp": 1653202716273,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "2ZNx8jYT3nl_",
    "outputId": "79194e1f-65c0-4683-d74c-0223fdc50001"
   },
   "outputs": [],
   "source": [
    "# We use the 'pip' package manager to install the 'biopython'\n",
    "# package.\n",
    "!pip -q install biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kil-54c4Me4"
   },
   "source": [
    "### #3 Prepare the data for the neural network\n",
    "\n",
    "#### Create a class to represent the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'random' because we need random number\n",
    "# generation.\n",
    "import random\n",
    "# Import 're' to deal with regular expressions.\n",
    "import re\n",
    "# Import 'sys' to print out error/log messages.\n",
    "import sys\n",
    "# Import the sequence I/O module from Biopython.\n",
    "from Bio import SeqIO\n",
    "# Import dataset utilities from PyTorch.\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "  \n",
    "    \"\"\"\n",
    "    This class is used to build the data set of promoter\n",
    "    sequences.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self,\n",
    "                 fasta_file,\n",
    "                 win_before,\n",
    "                 win_after,\n",
    "                 pos = 1000,\n",
    "                 n_neg = 1):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        To build it, you need to specify:\n",
    "\n",
    "        - A FASTA file where the sequences are stored.\n",
    "        - The length of the window before the TSS.\n",
    "        - The length of the window after the TSS.\n",
    "        - The position of the TSS.\n",
    "        - How many negative examples should be randomly drawn for\n",
    "          each positive example?\n",
    "        \"\"\"\n",
    "\n",
    "        # The TSS is at pos 'pos' so we extract the beginning\n",
    "        # of the window calculating the difference between\n",
    "        # 'pos' and 'win_before'.\n",
    "        tss = pos - win_before\n",
    "        \n",
    "        # The total window length is going to be the sum of the\n",
    "        # window length before the TSS and of the window length\n",
    "        # after the TSS, plus 1 since the TSS is 1 base long .\n",
    "        win_len = win_before + win_after + 1\n",
    "\n",
    "        # Initialize empty lists for inputs and targets.\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # For each sequence in the FASTA file\n",
    "        for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "\n",
    "            # Get the last position where you can start the\n",
    "            # window.\n",
    "            max_pos = len(seq_record.seq) - win_len - 1\n",
    "\n",
    "            # For each negative example that we want to pick\n",
    "            # from the sequence\n",
    "            for i in range(n_neg + 1):\n",
    "                \n",
    "                # Set 'k' equal to the TSS.\n",
    "                k = tss\n",
    "                \n",
    "                # If you are at the first negative example\n",
    "                if i == 0:\n",
    "                    \n",
    "                    # Create a tensor containing 1.0.\n",
    "                    target = torch.tensor(1.)\n",
    "                \n",
    "                # Otherwise\n",
    "                else:\n",
    "                    \n",
    "                    # Pick a random position as a negative example.\n",
    "                    target = torch.tensor(0.)\n",
    "                    \n",
    "                    # While 'k' is equal to the TSS\n",
    "                    while k == tss:\n",
    "                        \n",
    "                        # Update 'k' so that it is a random integer\n",
    "                        # between 0 and the last position where you\n",
    "                        # can start the window.\n",
    "                        k = random.randint(0, max_pos)\n",
    "\n",
    "                # Get the substring from 'k' to 'k' + 'win_len' and\n",
    "                # make it all uppercase.\n",
    "                s = str(seq_record.seq[k:k+win_len]).upper()\n",
    "\n",
    "                # If the substring contains only ACGT symbols.\n",
    "                if re.match(\"[^ACGT]\", s) is None:\n",
    "\n",
    "                    # Convert to a one-hot encoding and append the\n",
    "                    # example to the lists.\n",
    "                    self.inputs.append(one_hot_dna(s))\n",
    "                    self.targets.append(target)\n",
    "\n",
    "                # Otherwise\n",
    "                else:\n",
    "\n",
    "                    # Inform the user that the sequence will be ignored.\n",
    "                    warnstr = \\\n",
    "                        f\"The sequence '{s}' contains non-ACGT \" \\\n",
    "                        f\"symbols. Therefore, it will be ignored.\"\n",
    "                    sys.stdout.write(warnstr)   \n",
    "            \n",
    "    # The '__getitem__' method must be implemented for subclasses\n",
    "    # of 'torch.utils.data.Dataset'. This method is used to return\n",
    "    # a specific element of the dataset, given its index.\n",
    "    def __getitem__(self,\n",
    "                    index):\n",
    "        \n",
    "        return self.inputs[index], self.targets[index]\n",
    "    \n",
    "    # The __len__ method returns the length of the dataset.\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(seq):\n",
    "    \"\"\"Take a string (or a BioPython sequence like 'dna.seq') and\n",
    "    convert it to a 2D tensor representing a one-hot encoding of\n",
    "    the string.\n",
    "    \n",
    "    The tensor has dimensions [4, sequence_lenght].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : ``str``\n",
    "        A string representing a DNA sequence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hot_dna_tensor : ``torch.Tensor``\n",
    "        A tensor containing the one-hot encoding for the DNA\n",
    "        sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary from which the encoding of each letter\n",
    "    # will be taken.\n",
    "    hot_dna = \\\n",
    "        {\"A\":(1.,0.,0.,0.), \"T\":(0.,0.,0.,1.),\n",
    "         \"C\":(0.,1.,0.,0.), \"G\":(0.,0.,1.,0.),\n",
    "         \"N\":(0.,0.,0.,0.)}\n",
    "    \n",
    "    # Return the one-hot encoding for the given sequence.\n",
    "    return torch.tensor([hot_dna[c] for c in seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some options for the training and validation datasets and for using the GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2347,
     "status": "ok",
     "timestamp": 1653202718615,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "yeKR47OVfT0X"
   },
   "outputs": [],
   "source": [
    "# Import 'PyTorch'.\n",
    "import torch\n",
    "\n",
    "#------------------------------ Window -------------------------------#\n",
    "\n",
    "# Set the size of the window before the TSS.\n",
    "win_before = 100\n",
    "\n",
    "# Set the size of the window after the TSS.\n",
    "win_after = 20\n",
    "\n",
    "# Calculate the total window size (add one to the sum of the\n",
    "# before/after windows because the TSS is 1 base long).\n",
    "win_len = win_before+win_after+1\n",
    "\n",
    "#------------------------- Negative examples -------------------------#\n",
    "\n",
    "# The total number of promoter examples is around 29000.\n",
    "#\n",
    "# For each positive example, we generate 'n_neg' negative examples from\n",
    "# random places in the sequences.\n",
    "#\n",
    "# The total number of examples is, therefore, about 29000*(1+n_neg).\n",
    "n_neg = 1\n",
    "\n",
    "#--------------------- Training/validation sets ----------------------#\n",
    "\n",
    "# Set the fraction of the data that will be used as the training set\n",
    "# (the smaller the fraction, the faster the learning).\n",
    "train_fraction = 0.5\n",
    "\n",
    "# Set the fraction of the data that will be used as the validation set.\n",
    "validation_fraction = 0.05\n",
    "\n",
    "# Another example could be:\n",
    "# train_fraction = 0.8\n",
    "# validation_fraction = 1 - train_fraction\n",
    "\n",
    "# The size of the mini-batches used during training.\n",
    "batch_size = 256\n",
    "\n",
    "#---------------------------- GPU options ----------------------------#\n",
    "\n",
    "# Check whether a GPU is available.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use the GPU if available; otherwise, use the CPU.\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16831,
     "status": "ok",
     "timestamp": 1653202872711,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "jJWVH_V765Sk"
   },
   "outputs": [],
   "source": [
    "# Create a DNADataset with the sequences of interest and the specified\n",
    "# options.\n",
    "all_data = DNADataset(fasta_file = fasta_file,\n",
    "                      win_before = win_before,\n",
    "                      win_after = win_after,\n",
    "                      n_neg = n_neg)\n",
    "\n",
    "# Get the total number of examples.\n",
    "n_tot = len(all_data)\n",
    "\n",
    "# Get the indices (drawn randomly) for the examples that will go into\n",
    "# the training and validation datasets.\n",
    "indices = torch.randperm(n_tot)\n",
    "\n",
    "#------------------------- Tranining dataset -------------------------#\n",
    "\n",
    "# Get the number of examples that should be included in the training\n",
    "# dataset.\n",
    "n_train = int(train_fraction * n_tot)\n",
    "\n",
    "# Get the subset of the total data that will constitute the training\n",
    "# dataset.\n",
    "train_subset = \\\n",
    "    torch.utils.data.Subset(all_data,\n",
    "                            indices[:n_train])\n",
    "\n",
    "# Create a DataLoader object for the training dataset to automate the\n",
    "# use of mini-batches.\n",
    "train_loader = \\\n",
    "    torch.utils.data.DataLoader(train_subset,\n",
    "                                batch_size = batch_size)\n",
    "\n",
    "#------------------------ Validation dataset -------------------------#\n",
    "\n",
    "# Get the number of examples that should be included in the validation\n",
    "# dataset.\n",
    "n_validation = int(validation_fraction * n_tot)\n",
    "\n",
    "# Get the subset of the total data that will constitute the validation\n",
    "# dataset.\n",
    "validation_subset = \\\n",
    "    torch.utils.data.Subset(all_data,\n",
    "                            indices[n_train:n_train+n_validation])\n",
    "\n",
    "# Create a DataLoader object for the validation dataset to automate\n",
    "# the use of mini-batches.\n",
    "validation_loader = \\\n",
    "    torch.utils.data.DataLoader(validation_subset,\n",
    "                                batch_size = batch_size)\n",
    "\n",
    "# Sometimes, working with all test data as one large tensor is\n",
    "# convenient.\n",
    "validation_data = \\\n",
    "    torch.cat([validation_subset[i][0].unsqueeze(0) \\\n",
    "               for i in range(n_validation)]).to(device)\n",
    "validation_target = \\\n",
    "    torch.cat([validation_subset[i][1].unsqueeze(0) \\\n",
    "               for i in range(n_validation)]).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0iTMCOkTHai"
   },
   "source": [
    "### #4 Initialize the parameters for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653202876337,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "kpSges3ecbdz"
   },
   "outputs": [],
   "source": [
    "#---------------------- Fully connected network ----------------------#\n",
    "\n",
    "# Set the number of neurons in the first hidden layer.\n",
    "n_hidden1 = 10\n",
    "\n",
    "# Set the number of neurons in the second hidden layer.\n",
    "n_hidden2 = 5\n",
    "\n",
    "#----------------------- Convolutional network -----------------------#\n",
    "\n",
    "# The convolutional network will have a convolution layer and one\n",
    "# hidden layer with 'n_hidden1' hidden units.\n",
    "\n",
    "# Set the number of channels.\n",
    "n_channels = 10\n",
    "\n",
    "# Set the kernel size.\n",
    "kernel_size = 4\n",
    "\n",
    "#------------------------ Training procedure -------------------------#\n",
    "\n",
    "# Set the number of epochs.\n",
    "n_epochs = 50\n",
    "\n",
    "# Set the weight decay.\n",
    "weight_decay = 1.e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rGFo5KOotN"
   },
   "source": [
    "### #5 Define the convolutional neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1653202879312,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "nRMyuzjsnmfm"
   },
   "outputs": [],
   "source": [
    "# Import the neural network module from 'PyTorch'.\n",
    "import torch.nn as nn\n",
    "# Import the functional module from 'PyTorch'.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvolutionNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Class defining a convolutional neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 win_len,\n",
    "                 n_hidden,\n",
    "                 n_channels,\n",
    "                 kernel_size):\n",
    "        \n",
    "        # Initialize the network as its parent class.\n",
    "        super(ConvolutionNet, self).__init__()\n",
    "        \n",
    "        # Set the number of output units from the convolution.\n",
    "        self.n_conv_out = n_channels * (win_len - kernel_size+1)\n",
    "        \n",
    "        # It is a 2D kernel, because the input is (win_len, 4).\n",
    "        #\n",
    "        # Alternatively, one could use a 1D tensor of size\n",
    "        # 4 * win_len and a 1D convolution with stride 4.\n",
    "        #\n",
    "        # The 'kernel_size' is the number of nucleotides in\n",
    "        # the convolution.\n",
    "        self.conv1 = \\\n",
    "            nn.Conv2d(in_channels = 1,\n",
    "                      out_channels = n_channels,\n",
    "                      kernel_size = (kernel_size, 4),\n",
    "                      stride = 1,\n",
    "                      padding = 0)\n",
    "        \n",
    "        # Set a fully connected layer after the convolution layer.\n",
    "        self.fc1 = \\\n",
    "            nn.Linear(in_features = self.n_conv_out,\n",
    "                      out_features = n_hidden)\n",
    "        \n",
    "        # Set the final linear output layer.\n",
    "        self.fc2 = \\\n",
    "            nn.Linear(in_features = n_hidden,\n",
    "                      out_features = 1)\n",
    "     \n",
    "    def forward(self,\n",
    "                x):\n",
    "        \"\"\"Forward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The ReLU activation function is used on the hidden\n",
    "        # layers.\n",
    "        x = F.relu(self.conv1(x.unsqueeze(1))).view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Return the result of the forward pass.\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6 Select the network to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True if you want to use the convolutional network\n",
    "# instead.\n",
    "convolution = False\n",
    "\n",
    "# If 'convolution' is True\n",
    "if convolution:\n",
    "    \n",
    "    # The class defined above is used.\n",
    "    model = \\\n",
    "        ConvolutionNet(win_len = win_len,\n",
    "                       n_hidden = n_hidden1,\n",
    "                       n_channels = n_channels,\n",
    "                       kernel_size = kernel_size).to(device)\n",
    "\n",
    "# Otherwise\n",
    "else:\n",
    "    \n",
    "    # A standard feed-forward fully connected network is used.\n",
    "    # (the 'Flatten()' method is needed to make the input tensor\n",
    "    # 1-dimensional.\n",
    "    model = \\\n",
    "        torch.nn.Sequential(torch.nn.Flatten(),   \n",
    "                            torch.nn.Linear(win_len * 4, n_hidden1),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden1, n_hidden2),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden2, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRaFkBTIIm1G"
   },
   "source": [
    "### #7 Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22234,
     "status": "ok",
     "timestamp": 1653202904890,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "R3CYGf8_OrdU",
    "outputId": "8004cd8a-c4f1-483f-e267-b6b70fb1b6e4"
   },
   "outputs": [],
   "source": [
    "# Set the optimizer. We use the Adam optimizer here.\n",
    "optimizer = \\\n",
    "    torch.optim.Adam(model.parameters(),\n",
    "                     weight_decay = weight_decay)\n",
    "\n",
    "# Set the loss function. BCEWithLogitsLoss is used with\n",
    "# linear outputs and corresponds to a sigmoid activation\n",
    "# followed by a binary cross entropy loss.\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Move the target values to the GPU.\n",
    "v_target = validation_target.to(device)\n",
    "\n",
    "# Initialize a dictionary to store the performance values.\n",
    "performance = \\\n",
    "    {\"Epoch\" : [],\n",
    "     \"Train loss\" : [],\n",
    "     \"Train wrong\" : [],\n",
    "     \"Test loss\" : [],\n",
    "     \"Test wrong\": [],\n",
    "    }\n",
    "\n",
    "#------------------------------- Train -------------------------------#\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Save the epoch number and the train loss/train wrong\n",
    "    # performances (initialized to 0).\n",
    "    performance[\"Epoch\"].append(epoch)\n",
    "    performance[\"Train loss\"].append(0)\n",
    "    performance[\"Train wrong\"].append(0)\n",
    "    \n",
    "    # For each minibatch of inputs (x) and associated targets (t)\n",
    "    for x, t in train_loader:\n",
    "        \n",
    "        # Set the gradients to 0.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move the inputs and targets to the selected device.\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # Pass the inputs through the network and flatten the outputs.\n",
    "        y = model(x).flatten()\n",
    "        \n",
    "        # Calculate the loss between the network's outputs and the\n",
    "        # and the targets.\n",
    "        loss = loss_func(y, t)\n",
    "        \n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take an optimization step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Disable gradient calculation.\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Update the training loss.\n",
    "            performance[\"Train loss\"][-1] += \\\n",
    "                loss.item()*len(y) / n_train\n",
    "\n",
    "            # Count the number of outputs that are correct (with a\n",
    "            # threshold of 0.5).\n",
    "            y = torch.sigmoid(y)\n",
    "            performance[\"Train wrong\"][-1] += \\\n",
    "                (2*t-1).mul(2*y-1.).lt(0.0).sum().item() / \\\n",
    "                 n_train\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Pass the validation data through the network and flatten\n",
    "        # the outputs.\n",
    "        y = model(validation_data).flatten()\n",
    "        \n",
    "        # Save the test loss.\n",
    "        performance[\"Test loss\"].append(loss_func(y, v_target).item())\n",
    "        \n",
    "        # Count the number of outputs that are wrong.\n",
    "        y = torch.sigmoid(y)\n",
    "        \n",
    "        # (target-0.5)*(y-0.5) < 0 if the prediction is wrong\n",
    "        performance[\"Test wrong\"].append(\\\n",
    "            (v_target-0.5).mul(y-0.5).lt(0.0).sum().item() / \\\n",
    "             n_validation)\n",
    "    \n",
    "    # Print out the performances every 5 epochs.\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        \n",
    "        infostr = \\\n",
    "            f\"Epoch: {epoch+1}. \" \\\n",
    "            f\"Train wrong: {performance['Train wrong'][-1]}. \" \\\n",
    "            f\"Test wrong: {performance['Test wrong'][-1]}.\\n\"\n",
    "        \n",
    "        sys.stdout.write(infostr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #8 Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1653202905468,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "acmGxuC_j6ed",
    "outputId": "0251fc5e-be37-4928-a308-62b49112c44f"
   },
   "outputs": [],
   "source": [
    "# Import 'matplotlib' and 'seaborn' for plotting purposes.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import 'pandas' to manipulate the data.\n",
    "import pandas as pd\n",
    "\n",
    "# Set the 'seaborn' theme.\n",
    "sns.set()\n",
    "\n",
    "# Create a data frame from the dictionary where the\n",
    "# performances were saved.\n",
    "df = pd.DataFrame.from_dict(performance).set_index(\"Epoch\")\n",
    "\n",
    "# Generate and show a line plot with the loss.\n",
    "sns.lineplot(data = df,\n",
    "             dashes=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0pdXZWXiifz"
   },
   "source": [
    "### #9 Plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1653202905469,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "kNRUz8ADotYD"
   },
   "outputs": [],
   "source": [
    "# Import 'numpy'.\n",
    "import numpy\n",
    "\n",
    "def ROCandRoll(target,\n",
    "               output):\n",
    "    \"\"\"Take in input a tensor of target values (must be 0/1)\n",
    "    and prediction outputs and return a dictionary with the\n",
    "    false positives (FP) rate, the true positives (TP) rate,\n",
    "    the sorted outputs and the sorted targets.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Disable gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Get the length of the data passed from the\n",
    "        # length of the targets.\n",
    "        n = len(target)\n",
    "        \n",
    "        # Get the number of positive examples.\n",
    "        n_pos = target.sum()\n",
    "        \n",
    "        # Get the number of negative examples.\n",
    "        n_neg = n - n_pos\n",
    "        \n",
    "        # Sort the outputs.\n",
    "        z, order = torch.sort(output)\n",
    "        \n",
    "        # Sort the targets the same way.\n",
    "        target = target[order]\n",
    "        \n",
    "        # Get the true positives (TP). The cumulative counts\n",
    "        # of targets = 1 are the false negatives (FN) counts.\n",
    "        TP = n_pos - torch.cumsum(target, dim = 0)\n",
    "        \n",
    "        # Get the false positives (FP).\n",
    "        FP = torch.arange(n, 0, step = -1) - TP\n",
    "        \n",
    "        # Save the rates and sorted outputs/targets.\n",
    "        roc = \\\n",
    "            {\"TP rate\" : (TP / n_pos).numpy(),\n",
    "             \"FP rate\" : (FP / n_neg).numpy(),\n",
    "             \"Sorted outputs\": z.numpy(),\n",
    "             \"Sorted targets\" : target.numpy(),\n",
    "            }\n",
    "        \n",
    "        # Calculate and save the AUC.\n",
    "        roc[\"AUC\"] = \\\n",
    "            sum([0.5*(roc[\"TP rate\"][i] + \\\n",
    "                      roc[\"TP rate\"][i+1]) * \\\n",
    "                (roc[\"FP rate\"][i] - roc[\"FP rate\"][i+1]) \\\n",
    "                 for i in range(n - 1)])\n",
    "        \n",
    "        # Return the dictionary.\n",
    "        return roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1653202906020,
     "user": {
      "displayName": "Anders Krogh",
      "userId": "14176282564609376342"
     },
     "user_tz": -120
    },
    "id": "Y3S_6WWpjEI9",
    "outputId": "c4b81c2c-69ca-40a6-9d1d-a175b9f761b8"
   },
   "outputs": [],
   "source": [
    "# Disable gradient calculation.\n",
    "with torch.no_grad():\n",
    "  \n",
    "    # Get the outputs for all validation data\n",
    "      y = torch.sigmoid(model(validation_data).flatten()).cpu()\n",
    "      roc = ROCandRoll(validation_target, y)\n",
    "\n",
    "# Set the 'seaborn' theme.\n",
    "sns.set()\n",
    "\n",
    "# Generate a scatter plot because the line plot is very slow.\n",
    "sns.scatterplot(data = roc, \n",
    "                x = \"FP rate\",\n",
    "                y = \"TP rate\",\n",
    "                # Type of marker\n",
    "                marker = \".\",\n",
    "                # Size of the markers\n",
    "                s = 10,\n",
    "                # Set the border color for the markers\n",
    "                edgecolor = \"none\",\n",
    "                # Set the legend label for the markers\n",
    "                label = \"ROC curve\").set_title(\"ROC plot\")\n",
    "\n",
    "# Plot the AUC.\n",
    "plt.plot([0., 1.], [0., 1.], \n",
    "         # Width of the line\n",
    "         linewidth = 0.2,\n",
    "         # Color of the line\n",
    "         color = 'black')\n",
    "\n",
    "# Add the text with the AUC value.\n",
    "plt.text(0.3, 0.6, \n",
    "         \"AUC: \" + \"%0.3f\" % roc[\"AUC\"],\n",
    "         horizontalalignment = \"left\")\n",
    "\n",
    "# Decorate the ROC curve with the threshold values.\n",
    "\n",
    "# Define the cut-off.\n",
    "cut = 0.1\n",
    "\n",
    "# Initialize an empty list of lists storing the\n",
    "# points that will represent the threshold values.\n",
    "points = [[],[]]\n",
    "\n",
    "# For each threshold\n",
    "for i in range(n_validation):\n",
    "\n",
    "    # If the output is higher than the current threshold\n",
    "    if roc[\"Sorted outputs\"][i] > cut:\n",
    "\n",
    "        # Add the FP rate to the list of points.\n",
    "        points[0].append(roc[\"FP rate\"][i])\n",
    "\n",
    "        # Add the TP rate to the list of points.\n",
    "        points[1].append(roc[\"TP rate\"][i])\n",
    "\n",
    "        # Add the FP rate and the TP rate to the plot.\n",
    "        plt.text(roc[\"FP rate\"][i], \n",
    "                 roc[\"TP rate\"][i], \n",
    "                 \"%0.1f\" % cut,\n",
    "                 horizontalalignment = \"left\",\n",
    "                 verticalalignment = \"top\")\n",
    "\n",
    "        # Update the threshold value.\n",
    "        cut += 0.1\n",
    "\n",
    "# Generate a new plot on top of the previous one\n",
    "# showing the threshold values.\n",
    "sns.scatterplot(# Get the x-coordinates of the points\n",
    "                # to plot.\n",
    "                x = points[0],\n",
    "                # Get the y-coordinates of the points\n",
    "                # to plot.\n",
    "                y = points[1],\n",
    "                # Use a different type of marker.\n",
    "                marker = \"*\",\n",
    "                # Make the markers bigger.\n",
    "                s = 150,\n",
    "                # Make the markers completely opaque.\n",
    "                alpha = 1,\n",
    "                # Add a label for the markers to the\n",
    "                # legend.\n",
    "                label = \"Cut-off values\")\n",
    "\n",
    "# Show the complete plot.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4Be94HabeRMypLxUDMDIg",
   "collapsed_sections": [
    "uDN1s6SQ4AkF",
    "4Kil-54c4Me4",
    "l0iTMCOkTHai",
    "f_rGFo5KOotN",
    "VRaFkBTIIm1G",
    "h0pdXZWXiifz"
   ],
   "name": "TSSprediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
