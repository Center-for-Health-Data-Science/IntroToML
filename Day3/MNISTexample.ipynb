{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPO3DEkijCSN01FBxMlCfWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Make a neural network for reading handwritten digits\n","\n","Here a neural network is constructed and trained on MNIST data.\n","\n","It is a very simple neural network: Two hidden layers, fully connected.\n","\n","MNIST contains images of 28x28 pixels and a corresponding label of which number it is (0-9)."],"metadata":{"id":"6MoSL1m3497S"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"LizIxRzsurma","executionInfo":{"status":"ok","timestamp":1682485293711,"user_tz":-120,"elapsed":410,"user":{"displayName":"Anders Krogh","userId":"14176282564609376342"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["## Get the MNIST data"],"metadata":{"id":"_3quuFvI45MU"}},{"cell_type":"code","source":["import torchvision\n","# NOTE: ToTensor scales to [0,1] interval\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","mnist_full_train=torchvision.datasets.FashionMNIST(root='MNIST_dir', train=True, transform=transform, download=True)\n","\n","image_dim=mnist_full_train[0][0].shape[1]\n","print('Number of pixels in images:',image_dim,'x',image_dim)\n"],"metadata":{"id":"2RqWCG-qu8OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the first 1000 images for training\n","train_set = torch.utils.data.Subset(mnist_full_train, list(range(1000)))\n","\n","# Make a \"data loader\" that will return batches of images and labels\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n"],"metadata":{"id":"cz9VyfPsz05n","executionInfo":{"status":"ok","timestamp":1682483482241,"user_tz":-120,"elapsed":416,"user":{"displayName":"Anders Krogh","userId":"14176282564609376342"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Make the neural network\n","\n","We use the nn.Sequential, which puts the modules one after the other.\n","\n","Because the images are 2D tensors, the first module flattens the images to 1D - from 28x28 to an array of 784 numbers between 0 and 1 (gray scale)."],"metadata":{"id":"JqiCy4Qf51o1"}},{"cell_type":"code","source":["my_network = nn.Sequential(\n","    # First, flatten image\n","    nn.Flatten(),\n","    # One linear layer with 784 inputs and 100 units\n","    nn.Linear(image_dim*image_dim,100),\n","    # ReLU activation function\n","    nn.ReLU(),\n","    # Next layer must now have 100 inputs and we chose 50 units in this layer\n","    nn.Linear(100,50),\n","    nn.ReLU(),\n","    # Final layer must have 10 output units, one for each digit \n","    nn.Linear(50,10)\n","    )"],"metadata":{"id":"--qRJB0OvR95","executionInfo":{"status":"ok","timestamp":1682485301003,"user_tz":-120,"elapsed":431,"user":{"displayName":"Anders Krogh","userId":"14176282564609376342"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["## Set up the loss function and optimizer"],"metadata":{"id":"QkeV6laz8JYG"}},{"cell_type":"code","source":["# The cross entropy loss function combines softmax and loss into one\n","loss_function = nn.CrossEntropyLoss()\n","\n","# The optimizer must know which parameters to optimize\n","# The trainable parameters of a network is returned by the parameters() method\n","optimizer = torch.optim.Adam(my_network.parameters())\n"],"metadata":{"id":"slqzdeLL8TF5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"0ck-rkUW8WXP"}},{"cell_type":"code","source":["\n","# Here we run 50 epochs\n","for epoch in range(50):\n","    # Each time we call the train_loader it returns one batch of examples\n","    for image,label in train_loader:\n","        ### Standard training sequence that can be used always ################\n","        # Reset all the gradients to zero\n","        optimizer.zero_grad()\n","        # Get the outputs of the neural network for the batch\n","        y = my_network(image)\n","        # Calculate the loss\n","        loss = loss_function(y,label)\n","        # Do the back-propagation\n","        loss.backward()\n","        # Update the weights\n","        optimizer.step()\n","        ### End of standard sequence ##########################################\n","\n","    if (epoch+1)%10==0:\n","        print('Epoch:',epoch,'Loss',loss.item())\n","\n","\n"],"metadata":{"id":"Od7HkKEOvxfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check if results make sense"],"metadata":{"id":"Kn6SXFsb8bRT"}},{"cell_type":"code","source":["# Pick an arbitrary example from the training set\n","image,label = train_set[7]\n","y = nn.functional.softmax(my_network(image).squeeze(),dim=0)\n","print('Actual label',label)\n","print('Output after softmax:')\n","for i in range(10):\n","    print(i,': ','%.5f' % y[i].item())"],"metadata":{"id":"S0Fpes7L2G58"},"execution_count":null,"outputs":[]}]}